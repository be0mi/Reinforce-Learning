{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "import random as rd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode = 'rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x225c5c3f110>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, lr):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.memory = []\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0],128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(128,env.action_space.n)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class memory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen = capacity)\n",
    "    \n",
    "    def push(self, arr):\n",
    "        self.memory.append(arr)\n",
    "\n",
    "    def get_mini_batch(self, batch_size):\n",
    "        return rd.sample(self.memory,batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "def get_action(obs, eps):\n",
    "    global epslist,steps_done\n",
    "    eps = 0.05 + (0.9 - 0.05) * math.exp(-1. * steps_done / 1000)\n",
    "    steps_done += 1\n",
    "    epslist.append(eps)\n",
    "    if np.random.random() > eps:\n",
    "        with torch.no_grad():\n",
    "            return policy_model(torch.tensor(obs, device=device).unsqueeze(0)).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimization(target_model, policy_model, memory, batch_size):\n",
    "    global d_factor, loss\n",
    "    criterion = nn.HuberLoss().to(device)\n",
    "    optimizer = optim.Adam(policy_model.parameters(), lr = policy_model.lr)\n",
    "    observation_batch, action_batch, reward_batch, new_observation_batch = [], [], [], []\n",
    "    done_mask = deque([])\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    else:\n",
    "        batch = memory.get_mini_batch(batch_size)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        observation_batch.append(torch.tensor(batch[i][0], device=device))\n",
    "        action_batch.append(torch.tensor(batch[i][1],device=device))\n",
    "        reward_batch.append(torch.tensor(batch[i][3], device=device))\n",
    "        if batch[i][2] is not None:\n",
    "            new_observation_batch.append(torch.tensor(batch[i][2], device=device))\n",
    "            done_mask.append(torch.tensor(True))\n",
    "        else:\n",
    "            done_mask.append(torch.tensor(False))\n",
    "\n",
    "    observation_batch = torch.stack(observation_batch)\n",
    "    action_batch = torch.stack(action_batch)\n",
    "    reward_batch = torch.stack(reward_batch)\n",
    "    new_observation_batch = torch.stack(new_observation_batch)\n",
    "    # print(action_batch)\n",
    "    masked_state = torch.zeros(batch_size, device=device)\n",
    "    # print(new_observation_batch.shape)\n",
    "\n",
    "    # print((d_factor * target_model(torch.stack([batch[i][2] for i in range(batch_size)]).to(device))).size())\n",
    "    # print(policy_model(observation_batch.to(device)).shape, action_batch.shape)\n",
    "    y_hat = policy_model(observation_batch).gather(1, action_batch)\n",
    "    # print(y_hat.shape)\n",
    "    with torch.no_grad():\n",
    "        masked_state[done_mask] = target_model(new_observation_batch).max(1)[0]\n",
    "    \n",
    "    y = masked_state * d_factor + reward_batch\n",
    "    # print(reward_batch.shape)\n",
    "    # print(y.shape)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    cost = criterion(y_hat, y.reshape(-1,1))\n",
    "    loss.append(float(cost))\n",
    "    # print(cost)\n",
    "    # print(y_hat, y.reshape(-1,1),y.shape, y_hat.shape)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "epochs = 600\n",
    "d_factor = 0.95\n",
    "lr = 1e-4\n",
    "seta = 0.005\n",
    "steps_done = 0\n",
    "loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_model = DQN(lr = lr).to(device)\n",
    "target_model = DQN(lr = lr).to(device)\n",
    "target_model.load_state_dict(policy_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0105],\n",
       "        [ 0.0098],\n",
       "        [-0.0317],\n",
       "        [-0.0117]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(env.reset()[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599 10\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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